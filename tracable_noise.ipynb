{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Experiment setup:\n",
    "# First, train a model on the original training data\n",
    "# Second, train a model with noise on the 0-250ms region\n",
    "# Third, train a model with noise on the 250-500ms regions\n",
    "# Lastly, take 5-10 SHAP approximation per model (depends on the computing time)\n",
    "# And make sure the SHAP per participant uses the same fixed sets for the approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.eegnet_variance import EEGNetMultiHeaded\n",
    "from pytorch_lightning.loggers import CometLogger\n",
    "from src.datamodule import DataModule, predictDataSet\n",
    "from src.preprocessing import create_dataset\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from src.preprocessing import *\n",
    "from src.noise_addition import zero_signal, add_gaussian_noise\n",
    "from src.model.variance_wrapper import EEGVarianceWrapper\n",
    "import copy\n",
    "import random\n",
    "import pickle\n",
    "import torch\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comet_logger = CometLogger(\n",
    "    api_key=\"WSATCNWE43zphHslQCTsJKcgk\",\n",
    "    workspace=\"marwo22\",  # Optional\n",
    "    project_name=\"bachelors-project\"  # Optional\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_guassian_noise_to_dataset(dataset, severity_index, low: int = 0, high: int = 308):\n",
    "    if (severity_index == 0):\n",
    "        return dataset\n",
    "\n",
    "    length = len(dataset[0])\n",
    "        # Add noise to 7.5%  * severityIndex of samples in the dataset. It can thus range from 5% to 50%\n",
    "    episodes_to_corrupt = random.sample(range(0, length), int(0.1 * severity_index * length))\n",
    "        # Add noise to the samples\n",
    "    for episode in episodes_to_corrupt:\n",
    "        channels_to_corrupt = int(64 / 10 * severity_index)\n",
    "        dataset[0][episode] = add_gaussian_noise(dataset[0][episode], 2 * severity_index, channels_to_corrupt, low, high)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_zero_to_dataset(dataset, severity_index, low: int = 0, high: int = 308):\n",
    "    if (severity_index == 0):\n",
    "        return dataset\n",
    "    \n",
    "    length = len(dataset[0])\n",
    "        # Add noise to 7.5%  * severityIndex of samples in the dataset. It can thus range from 5% to 50%\n",
    "    episodes_to_corrupt = random.sample(range(0, length), int(0.1 * severity_index * length))\n",
    "        # Add noise to the samples\n",
    "    for episode in episodes_to_corrupt:\n",
    "            # Can range from 5-50%\n",
    "        channels_to_corrupt = int(64 / 20 * severity_index)\n",
    "            # Zeroes the entire signal\n",
    "        dataset[0][episode] = zero_signal(dataset[0][episode], channels_to_corrupt, low, high, 100)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = [[] for _ in range(3)]\n",
    "\n",
    "for j in range(6):  \n",
    "    # Load in the datasets with the current participant as test set\n",
    "    train, val, test = create_dataset('./src/pickle_df', j + 1)\n",
    "\n",
    "    for i in range(3):\n",
    "        # Take deep copies\n",
    "        train_copy = copy.deepcopy(train)\n",
    "        val_copy = copy.deepcopy(val)\n",
    "        test_copy = copy.deepcopy(test)\n",
    "        \n",
    "        # Add noise to the deep copies. If i is 0, noise is added to the lower end, when it is 1, it is added to the higher end\n",
    "        # For this experiment, the intensity value is 5, resulting in a 'medium' level of noise\n",
    "        if i != 0:\n",
    "            train_copy = add_guassian_noise_to_dataset(train_copy, 10, low = (i - 1) * 128, high = i * 128 - 1)\n",
    "            val_copy = add_guassian_noise_to_dataset(val_copy, 10, low = (i - 1) * 128, high = i * 128 - 1)\n",
    "            test_copy = add_guassian_noise_to_dataset(test_copy, 10, low = (i - 1) * 128, high = i * 128 - 1)\n",
    "\n",
    "        dm = DataModule(train=train_copy, val=val_copy, test=test_copy, batch_size=16)\n",
    "\n",
    "        model = EEGNetMultiHeaded(chunk_size=308,\n",
    "                                num_electrodes=64,\n",
    "                                dropout=0.5,\n",
    "                                kernel_1=64,\n",
    "                                kernel_2=16,\n",
    "                                F1=8,\n",
    "                                F2=16,\n",
    "                                D=2,\n",
    "                                num_classes=2)\n",
    "        # Train for 25 epochs for this example\n",
    "        # Final one for results wil run for 50 most likely\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=25,\n",
    "            logger=comet_logger\n",
    "        )\n",
    "\n",
    "        # Fit and test model\n",
    "        trainer.fit(model, datamodule=dm)\n",
    "\n",
    "        test_shap = list()\n",
    "\n",
    "        shap_test_tensor = torch.empty((0, 1, 64, 308), dtype=torch.float32)\n",
    "\n",
    "        for (data, _) in dm.test_dataloader():\n",
    "            shap_test_tensor = torch.cat((shap_test_tensor, data), 0)\n",
    "\n",
    "        background = shap_test_tensor[:-10]\n",
    "        test_episodes = shap_test_tensor[-10:]\n",
    "\n",
    "        variance_model = EEGVarianceWrapper(model)\n",
    "\n",
    "        e_variance = shap.DeepExplainer(variance_model, background)\n",
    "\n",
    "        shap_values_variance = e_variance.shap_values(test_episodes)\n",
    "\n",
    "        shap_values[i].append(shap_values_variance)\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    # Just to visualize progress easier, the output is very messy\n",
    "    print(\"\\n\\n\\n\\n\\nFinished\" + str(j) + \"\\n\\n\\n\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
