\section{Discussion}\label{sec:discussion}

In this section, the results of the experiments, as presented earlier, will be discussed, and problems with the current experimental setup and points for improvement, as well as opportunities for further research, will be addressed.

\subsection{Uncertainty quantification}

The global and local noise experiment results, paired with the skew in the distribution of predicted variances on incorrect predictions, show that the model is behaving as expected. It is actively basing its variance prediction on the quality of the data and captures the artificially added noise as uncertainty in the variance output head. The successful quantification of heteroscedastic aleatoric uncertainty can be of many benefits in real-world applications. Firstly, the predicted uncertainty can be used as a sanity check of the model. A popular method of adjusting the desired specificity and sensitivity on classification tasks is to adjust the thresholds of the required probabilities for each class \citep{chen2006decision}. Something similar can be applied to a model that quantifies uncertainty. Some tasks may benefit from setting a threshold for the predicted uncertainty. All predictions with an uncertainty below this threshold are deemed trustable, and all predictions with higher uncertainties are discarded. Although this may work appropriately with just the quantification of aleatoric uncertainty, it may be beneficial to combine it with methods such as Bayesian networks to quantify the epistemic uncertainty, such that the uncertainty inherent in the model is also captured. Such a threshold may not benefit applications for BCIs such as this paper. However, this increase in accuracy is likely desired for tasks such as clinical diagnoses.

Successful quantification of aleatoric uncertainty can also benefit real-time classification applications for capturing problems with data gathering. For example, in the case of EEG classifications for BCIs, a constant high uncertainty can indicate that something is wrong with the electrodes used to record the EEGs.

\subsection{Uncertainty traceability}

The results of the fourth and final experiment on the traceability of the noise using Shapley values shows that, although noise has a visible effect on the resulting Shapley values, the location cannot be traced with these Shapley values. Interestingly, there is some weird behavior present. In the Shapley values originating from the two noisy Gaussian models, there is a peak visible that is almost identical for both noisy models, which is not present in the original model. This is also seen in the models with zeroed sections in the data, although the peaks are not identical here. However, they are very much different from the peak in the original signal. There is likely some relation between the two noisy signals that we do not know about that results in this behavior. 

The results from this experiment show that, despite using explainable AI to try and understand the behavior of our model, it still behaves a lot like a \verb|black box|. The first experiments showed that the behavior of the model's output was as expected. Adding additional sources of uncertainty increases the predicted uncertainty, which was as expected. Adding the noise on sections that held the essential features of the ErrP signal also increased the predicted uncertainty, which was as expected. However, when trying to understand the internal behavior of the model using explainable AI, it is seen that the model does not work at all as expected and shows very surprising behavior. This indicates that the model still acts like a \verb|black box|. All visible behavior works as intended, but the internal behavior of the model is still very unexpected and unknown.

\subsection{Experimental setup issues}

The dataset used for these experiments consisted of only six participants. Thus far, all experiments have been conducted using the same 5 participants as training data and the same participant for testing. This was done deliberately, as EEG signals are prone to high inter-participant variability. Due to this high variability and the low sample size of only six participants, testing on all participants was not a viable choice due to the low consistency in results between participants. Due to this high variance, it was impossible to gather good results, and the difference between training sessions is much more significant than the already quite large difference experienced with only one test participant.

The two methods of generating artificial noise, Gaussian noise and the zeroing of channels, used for the experiments are only rough simulations of noise and could be more realistic. Other types of noise common in EEG data could be simulated to achieve a more realistic situation. One such approach would be the recreation of physiological artifacts, such as the signals originating from the motor complex for actions like blinking. This method significantly increases the experiment's complexity and is beyond the scope of this research paper, but it could be interesting to research in the future.

For this experiment, we focused on a single model: EEGNet, developed by \cite{lawhern2018eegnet}. This model is a compact Convolutional Neural Network (CNN). However, this is one of many model architectures that can be used to classify EEGs. Different model architectures result in different behavior of the quantification and localization of heteroscedastic aleatoric uncertainty since they observe and train from the data differently. Thus, they can be interesting topics of research for the future.
\subsection{Conclusion}

In this paper, we proposed a model that can classify Error-Related Potentials and predict the uncertainty associated with this prediction. We explored the possibility of combining this model with a method from Explainable AI to localize the source of this uncertainty. To test this, we intentionally invoked uncertainty in the model by introducing artificial noise to the EEG data. Using various experiments, we tested whether the model could quantify its uncertainty and whether this uncertainty could be localized. We found that the proposed method can successfully quantify the uncertainty associated with its prediction, with noisy data resulting in higher predicted uncertainty. The localization of uncertainty did not show significant success, and the sources of uncertainty remained elusive. Our findings indicate that the quantification of uncertainty works as expected and can be used to check the trustability of predictions. Our findings also indicate that using our method of Explainable AI does not clearly indicate the source of uncertainty and that the internal workings of the model still remain unknown: a \verb|black box|.

We hope our findings inspire further research on the localization of sources of uncertainty. We desire that other researchers continue to explore this possibility and delve deeper into understanding how uncertainty can be precisely identified and localized. Ultimately, we hope to see our findings challenged and proven wrong in the future, as it would signify a significant advancement in the field and establish that the localization of uncertainty is achievable.
