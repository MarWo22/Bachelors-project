\section{Discussion}\label{sec:discussion}

In this section, the results of the aforementioned experiments will be discussed, and problems with the current experimental setup and points for improvement, aswell as further research will be adressed

\subsection{Uncertainty quantification}

The first experiment showed that the model predicts high variance on incorrect classifications. With the extremely high variances pointing towards random guession of the model. This observation allows for an interesting implementation of a sanity check for the model. This does not only apply for these specific EEG and BCI classification models, but to classification models in general. One could put a threshold on the maximum variance which is allowed in a prediction. All classifications with a predicted variance below this threshold are accepted, whereas all classifications with variance exceeding this threshold are rejected. This can drastically improve the accuracy of the model. However, this does come at the cost of rejecting some classifications, which would need manual classification for example. This can be a worthwhile trade-of for classification tasks in which false positives and false negatives have to be avoided at all costs, such as medical classifications.

The experiment in which the labels were shuffled showed an increase in the predicted variance as the percentage of labels shuffled increases. The second experiment, which shows that the addition of noise, in the form of Gaussian noise and the zoering of channels, on the entire length of the signal results in an increase in variance. These two experiments give a good indication that the model is behaving as expected, and is actively basing its variance prediction on the quality of the input data, and is capturing the artifically added heteroscedastic aleatoric uncertainty.

The third experiment shows that the noise location affects the predicted variance, with noise on the 250-500ms region resulting in higher variance and lower accuracy than noise on the 0-250ms region. However, this difference is small. Previous research shows that this was to be expected. Experiments with varying lengths of the input windows showed that there was only a $6\%$ increase when increasing the window from $300$ms to $600$ms after the event's presentation \citep{correia2021error}. Thus, the model was already capable of predicting the label accurately. However, the difference in accuracy and variance between noise in the two regions confirms that the model shifts its attention to the $250-500$ ms region when it has access to it, since noise in this range has a stronger effect. This aligns with the prominent features of the Error-related potential.

From the the localized and global noise experiment, one can observe that the type of noise added to the signal matters in the variance and accuracy of the model. This is to be expected. Zeroing has a larger effect, since the zeroed signal has no resemblance to the original signal, and thus the original signal cannot still effect the model. Whereas in Gaussian noise, the signal still has some resemblance to the original signal, regardless of the intensity of the Gaussian noise. The original signal will still effect the model.  

\subsection{Uncertainty traceability}


The fourth and final experiment shows whether the origin of noise can be traced down by using explainable AI, in the form of Shapley values. From this, we can observe that there is a clear difference between the oriinal signals, and the signals with artificially added noise. There is also a clear indication that the type of noise has an effect on the resulting Shapley values, with zeroing having an overal smoother uncertainty distribution, whereas the Gaussian has a rougher uncertainty distribution. Although there is a clear difference between all these signals, it is nearly impossible to accurately distinguish the location of the noise, purely based from the Shapley values. The difference between the original and the two noisy signals is easy to spot for the Gaussian noise, butt he difference between the two noisy regions is almost impossible to distinguish.

\subsection{Problems with experimental setup}

As one may have noticed in this paper, the dataset consists of data from six different participants. One may have also noticed, that thus far we have been training on the same participants (2-6), and test on the same participant (1). This was done deliberately. EEG signals are prone to a very high inter-participant variability. Due to this high variability, and only having six participants, testing on all participants was not a viable choice due to the low consistency in results between participants. Due to this high variance it was impossible to gather concise results, and the difference between training sessions are much larger than the already quite large difference that is experienced with only one test participant.

\subsection{Noise}

The two methods of generating artificial noise, Gaussian noise and the zeroing of channels, used for the experiments are only rough simulations of noise and could be more realistic. The noise used in these experiments, which showed results are magnitudes higher than what is encountered in real-life applications. A more realistic approach would be adding noise variants that are more common in EEG signals. One such approach would be the recreation of physiological artifacts, such as the signals originating from the motor complex for actions like blinking. This method drastically increases the experiment's complexity and is way beyond the scope of this research paper, but it can potentially be interesting to research in the future.

\subsection{Model}

For this experiment, I put the focus on a single model: EEGNet, developed by \cite{lawhern2018eegnet}. This model is a compact Convolutional Neural Network (CNN). However, this is not the only model architecture which is commonly used for EEG classification. One of these architectures which is commonly used is the Recurrent Neural Network (RNN). It is possible that the localization of the noise can be achieved when using the other architectures, since they observe and train from the data in different ways, and thus can be an interesting research for the future.
