\begin{abstract}
    {Accurate estimation of uncertainty plays a pivotal role in enhancing the reliability and interpretability of predictive models. This paper builds upon previous methods by proposing a novel approach that not only quantifies its uncertainty but also traces the sources of this uncertainty using a method from Explainable AI. In this study, a previously developed classification model for electroencephalogram signals is modified by incorporating an additional head predicting uncertainty. The contribution of features to the uncertainty prediction can be calculated, ultimately pointing towards the source of uncertainty. The proposed approach is evaluated on a dataset of Error-Related Potentials. To test this methodâ€™s effectiveness, the signals have been modified by introducing artificial noise in specific regions to challenge the prediction and localization of the noise. The results show that the modified model is capable of predicting uncertainty with a high degree of accuracy. However, the tracing of sources of uncertainty proves to be challenging despite the accurate uncertainty estimation, and the exact sources of uncertainty remain elusive. \vspace{6ex}}
    \end{abstract}
    