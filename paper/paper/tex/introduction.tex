\section{Introduction}\label{sec:introduction}

Deep neural networks have emerged as the state-of-the-art method for various machine learning tasks, exhibiting remarkable performance across domains such as computer vision, speech recognition, natural language processing, and even biomedical applications \citep{yoo2015deep, zheng2015investigating}. However, because of their excellent performance, the outputs of these networks are often blindly trusted and presumed to be accurate, which is not always valid as the networks are generally overconfident. Generally, a network has no way of communicating how certain the model is with its predictions. Proper quantification of these uncertainties is becoming increasingly important in the practical field of machine learning. Uncertainty quantification gives the user a better insight into a model's prediction.

\subsection{Uncertainty in machine learning}

In machine learning, uncertainty refers to the lack of confidence or knowledge in the predictions or outcomes produced by a model. It can arise due to various factors, such as limited or noisy data, model assumptions, or the underlying problem's complexity. The uncertainty of a model can be devided in to two types: aleatoric and epistemic uncertainty. Aleatoric uncertainty is the noise inherent in the data itself. It captures the inherent variability and randomness in the observed data, and is typically caused by noise and measurement errors. This type of uncertainty cannot be reduced with more data or better model training.

Aleatoric uncertainty can be split up further into two types. Homoscedastic aleatoric uncertainty is uncertainty which is constant over all inputs. Heteroscedastic aleatoric uncertainty depends on the given input and differs between episodes. For example, homoscedastic aleatoric uncertainty can be sensor noise, constant across all episodes. In contrast, the heteroscedastic aleatoric uncertainty can be a disconnected or faulty sensor, which only appears in a few episodes.

\subsection{Uncertainty quantification}

The quantification of unceratinty has been a subject of extensive research, with numerous previous studies proposing methods for the measuring and quantification of these uncertainties

One of the most studied approaches for modeling uncertainties is Bayesian Deep Learning Methods \citep{kendall2017uncertainties, chen2020uncertainty}. These models represent the weights as probability distributions rather than fixed values. The weights are treated as random values with prior distributions. After observing the data, the prior distributions are updated using Bayes' rule to obtain a posterior. The posterior can be used to make predictions, predicting a probability distribution rather than a fixed estimate. The spread of this final distribution indicates the uncertainty of the model.

\cite{nix1994estimating} suggested a different approach, initially designed for regression tasks. Their strategy was to use a non-bayesian network with two output heads. One predicts the target mean $\mu$, and one predicts the variance $\sigma$ with the variance capturing the heteroscedastic aleatoric uncertainty of the model. The model will learn to predict the variance indirectly with the loss function, with erroneous predictions getting punished less if the predicted variance is high. When using a sampling softmax method and a different loss function , this approach can also be used for classification \citep{kendall2017uncertainties}.

A different method of capturing predictive uncertainty involved deep ensembles \citep{lakshminarayanan2017simple}. The core idea behind deep ensembles is to have multiple models, with each trained independently on the same dataset. The variance between the different models can then be used to estimate the uncertainty. The variance reflects the level of disagreement between the models, which can be used to measure the overall uncertainty.

Monte Carlo Dropout is another method for predicting uncertainty \citep{gal2016dropout}. This method sets random weights to zero, canceling out their contribution. Multiple passes will be done with random dropouts, resulting in an output distribution with which it is possible to calculate the uncertainty.

These methods can all quantify uncertainty, each with its strenght and weaknesses. These methods can all answer the question of how ucnertain a model is. However, they cannot answer the question why the model is uncertain, and where this uncertainty comes from. Knowing the source of uncertainty can be very useful in real-world applications. For example, a unusual high source of uncertainty coming from one sensor, can be an indication that the sensor in question is faulty, and has to be replaced. Without this localization of uncertainty, this may have been overlooked, and ultimately affected the performance of the model. An approach to the localization of such sources of uncertainty is to introduce methods of Explainable AI. By calculating the Shapley Values of the model, the contribution of the features to the uncertainty can be approximated \citep{merrick2020explanation}. Ultimately, a cluster of features with a high contribution to the uncertainty, is a source of uncertainty. 

\subsection{Error-related potentials}

The medical field is one of the scientific fields that can benefit the most from uncertainty quantification. Generally, tasks such as medical diagnosis are of great importance and significance. However, data collection methods in this field, such as an electroencephalogram (EEG) or electrocardiogram  (ECG), are very prone to noise. In the case of EEGs, artifacts, such as signals from blinking or malfunctioning electrodes, can easily corrupt EEG data.

BCIs are systems that use the brain's electrical activity to extract the intention from a user and translate it into computer instructions. These systems are helpful in fields such as prosthetics, where the user's intent can be translated into the movements of said prosthetic. Recently, error-related potentials (ErrP) have become a topic of interest. An ErrP signal is a response to error perception after feedback. These signals spark interest for BCIs since they indicate an erroneous extraction of intention, in which case the system can reconsider the translated action. Alternatively, the erroneous episode can be used as a training example to improve future performance. 

Previous research has shown promising results in predicting ErrP signals \citep{correia2021error}. Specialized machine learning architectures designed explicitly for EEG signals have been designed and can be used for these tasks \citep{lawhern2018eegnet}. However, these models all lack a form of uncertainty quantification.

\subsection{Proposed method}

In this paper, I propose a multi-headed Convolutional Neural Network classifying EEG signals on ErrP signals. The proposed model has two output nodes. One outputting the classification label and one outputting an estimate of the uncertainty. Furthermore, Shaply Values will be integrated into the model to indicate which features contribute the most to the uncertainty estimate, explaining the sources of uncertainty. The model will be trained and tested on the Monitoring Error-Related Potential dataset \citep{chavarriaga2010learning}.