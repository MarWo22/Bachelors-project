\section{Introduction}\label{sec:introduction}

Deep neural networks have emerged as the state-of-the-art method for various machine learning tasks, exhibiting remarkable performance across domains such as computer vision, speech recognition, natural language processing, and even biomedical applications \citep{yoo2015deep, zheng2015investigating}. However, because of their excellent performance, the outputs of these networks are often blindly trusted and presumed to be accurate, which is not always valid as the networks are generally overconfident. Generally, a network has no way of communicating how certain the model is with its predictions. Proper quantification of these uncertainties is becoming increasingly important in the practical field of machine learning. Uncertainty quantification gives the user a better insight into a model's prediction.

\subsection{Uncertainty in machine learning}

In machine learning, uncertainty refers to the lack of confidence or knowledge in the predictions or outcomes produced by a model. It can arise from various factors, such as limited or noisy data, model assumptions, or the complexity of the underlying problem. The uncertainty of a model can be divided into two types: epistemic and aleatoric uncertainty. Epistemic uncertainty arises from the lack of knowledge or understanding of the underlying data distribution or model parameters. This type of uncertainty can often be reduced by feeding the model more training data. Aleatoric uncertainty is the noise inherent in the data itself. It captures the inherent variability and randomness in the observed data and is typically caused by noise and measurement errors. This type of uncertainty cannot be reduced with more data or better model training.

Aleatoric uncertainty can be split up further into two types. Homoscedastic aleatoric uncertainty is uncertainty that is constant over all inputs. Heteroscedastic aleatoric uncertainty depends on the given input and differs between episodes. For example, homoscedastic aleatoric uncertainty can be sensor noise, constant across all episodes. In contrast, the heteroscedastic aleatoric uncertainty can be a disconnected or faulty sensor, which only appears in a few episodes.

\subsection{Uncertainty quantification}

The quantification of uncertainty has been a subject of extensive research, with numerous previous studies proposing methods for measuring and quantifying these uncertainties.

One of the most studied approaches for modeling uncertainties is Bayesian Deep Learning Methods \citep{kendall2017uncertainties, chen2020uncertainty}. These models represent the weights as probability distributions rather than fixed values. The weights are treated as random values with prior distributions. After observing the data, the prior distributions are updated using Bayes' rule to obtain a posterior. The posterior can be used to make predictions, predicting a probability distribution rather than a fixed estimate. The spread of this final distribution indicates the uncertainty of the model.

\cite{nix1994estimating} suggested a different approach, initially designed for regression tasks. Their strategy was to use a non-bayesian network with two output heads. One predicts the target mean $\mu$, and one predicts the variance $\sigma$ with the variance capturing the heteroscedastic aleatoric uncertainty of the model. The model will learn to predict the variance indirectly with the loss function, with erroneous predictions getting punished less if the predicted variance is high. When using a sampling softmax method and a different loss function, this approach can also be used for classification \citep{kendall2017uncertainties}.

A different method of capturing predictive uncertainty involved deep ensembles \citep{lakshminarayanan2017simple}. The core idea behind deep ensembles is to have multiple models, each trained independently on the same dataset. The variance between the different models can then be used to estimate the uncertainty. The variance reflects the level of disagreement between the models, which can be used to measure the overall uncertainty.

Monte Carlo Dropout is another method for predicting uncertainty \citep{gal2016dropout}. This method sets random weights to zero, canceling out their contribution. Multiple passes will be done with random dropouts, resulting in an output distribution with which it is possible to calculate the uncertainty.

These methods can all quantify uncertainty, each with its strengths and weaknesses. These methods can all answer the question of how uncertain a model is. However, they cannot answer why the model is uncertain and where this uncertainty comes from. Knowing the source of uncertainty can be very useful in real-time applications. For example, an unusually high source of uncertainty from one sensor can indicate that the sensor in question is faulty and must be replaced. Without this localization of uncertainty, this may have been overlooked, which could have affected the model's performance. An approach to localizing such sources of uncertainty is introducing methods of Explainable AI. By calculating the model's Shapley Values, the features' contribution to the uncertainty can be approximated \citep{merrick2020explanation}. Ultimately, a cluster of features with a high contribution to the uncertainty is a source of uncertainty. 

\subsection{Error-related potentials}

The medical field stands to greatly benefit from the quantification of uncertainty, especially in tasks of great importance and significance, such as medical diagnoses. However, data collection methods in this field, such as an electroencephalogram (EEG), are highly susceptible to noise. EEGs involve placing electrodes on the scalp to detect the small electric signals originating from the brain's neurons. They provide a high temporal resolution, allowing researchers to study various aspects of the function of the brain, such as cognitive processes. However, due to the sensitive nature of this method, it introduces various sources of noise and interference. For example, small interferences, such as the frequencies originating from the AC powerlines and physiological artifacts like blinking and muscle movements, can contaminate the recorded signals, posing a challenge for accurate analysis.

A common use for the analysis of EEGs is the field of Brain-Computer Interfaces (BCIs). These systems use the brain's electrical activity to extract intention from a user and translate it into computer instructions. These systems are helpful in fields such as neuroprosthetics, where the user's intent can be translated into the movements of said prosthetic. Recently, Error-Related Potentials (ErrP) have become a topic of interest within this field. An ErrP signal is a response to error perception after feedback. These signals spark interest for BCIs since they indicate an erroneous extraction of intention, in which case the system can reconsider the translated action. Alternatively, the erroneous episode can be used as a training example to improve future performance. 

Two crucial channels for studying Error-Related Potentials are \verb|FZc|, located at the midline frontal scalp, and \verb|Cz|, located at the midline central scalp. ErrPs exhibit a characteristic waveform as seen in figure \ref{fig:FcZ}, typically showing a negative peak at around 200ms after the erroneous event, followed by a positive peak at approximately 300ms.

Previous research has shown promising results in predicting ErrP signals \citep{correia2021error}. Specialized machine learning architectures designed explicitly for EEG signals have been designed and can be used for these tasks \citep{lawhern2018eegnet}. This model, and most other models for classifying EEGs, are Convolutional Neural Networks (CNNs). A CNN uses convolutional layers designed to capture local patterns in the input data while pooling layers help reduce dimensionality and extract import features. In the context of EEGs, they can learn discriminateive representations directly from the raw EEG data, allowing them to classify and detect specific patterns. They have shown excellent accuracy in various EEG-based tasks such as brain-computer interfaces \citep{cecotti2010convolutional} and seizure detection \citep{zhou2018epileptic}.

\subsection{Proposed method}

This paper proposes a multi-headed Convolutional Neural Network classifying EEG signals on Error-Related Potentials. The proposed model has two output nodes. One outputting the classification label and one outputting an estimate of the model's uncertainty associated with that classification. This model will be tested on the Monitoring Error-Related Potential dataset \citep{chavarriaga2010learning}, to which artificial noise is added to challenge the model's ability to predict uncertainty. Furthermore, this model will be paired with a method from Explainable AI called Shapley values to approximate the contribution of features to the uncertainty prediction. Using this method, we test whether it is possible to use Explainable AI to locate the source of the artificial noise.