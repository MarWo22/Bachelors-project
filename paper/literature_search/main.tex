\documentclass[10pt]{article}

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{times}
\usepackage[a4paper]{geometry}

\title{{\bf Bachelor's Thesis} \\ Literature search}
\author{}
\date{\today}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\maketitle

\section*{The main four papers I will use for my thesis:}

\subsection*{Error perception classification in Brain-Computer Interfaces using CNN \cite{correia2021error}}

The main goal of this paper was to implement a CNN to predict error potentials which could be used for error correction in Brain-Computer Interfaces (BCIs). Error-related Potentials (ErrP) have been used to include the perception of subjects on errors caused by these BCIs. ErrPs originate from the anterior cingulate cortex and are elicited as a response to error perception after a feedback presentation. It used the publicly available dataset from the BNCI Horizon 2020 project website under the name Monitoring error-related potentials. The data was generated by subjects who monitored a moving cursor without controls. There was a 20\% chance for the cursor to move the wrong way. The only preprocessing that was done was a 1 to 10hz Butterworth bandpass filter. All 64 channels were used in windows of 600ms after the stimuli. This gave better results than the two selected channels (FCZ and FZ). They achieved an accuracy of 80\%, 76\% sensitivity, and 85\% specificity. 

I will use the same dataset this author used for my thesis since this dataset is the best for the ErrP classification and only uses 'mental' tasks without physical movements, so there will be fewer physiological artifacts.

\subsection*{Estimating the Mean and Variance of the Target Probability Distribution \cite{nix1994estimating}}
This paper aims to develop a feed-forward network that can also predict its uncertainty in a regression task. To achieve this, they created a network with two heads, each head having its own hidden layers. They only share the input layer without any shared hidden layers. For this particular task, it showed better performance. First, only the weights of the y-headed side (the actual regression task) were trained, and the $\sigma^2$ (Variance) head was ignored. The reasoning is to reduce the computing load since training the variance predictor does not make sense if the regression task is nowhere near accurate.

This paper aims for regression tasks with a clear difference between the predicted and actual continuous values. This strategy becomes more challenging in a classification task, but the following paper has some solutions to this problem.

\subsection*{What uncertainties do we need in bayesian deep learning for computer vision \cite{kendall2017uncertainties}}

Deep learning models have two major types of uncertainty. The first one is epistemic uncertainty. This is the uncertainty of the model parameters. This can often be explained away, given enough data. The other type is aleatoric uncertainty. This is noise inherent to the observations, such as faulty sensors. The latter can be further categorized into homoscedastic uncertainty, constant regardless of the input, and heteroscedastic uncertainty, which depends on the inputs. The latter sparks the most interest in my research topic since I want to track down artifacts in the signals. These uncertainties can be modeled using Bayesian deep learning approaches. To capture epistemic uncertainty in NNs, a prior distribution is put over all weights, such as a Gaussian distribution. These are called Bayesian Networks. They try to capture how much these weights vary on the data to model this uncertainty. For the aleatoric uncertainty, a distribution is placed over the model's output, and then try to learn the noise's variance based on the inputs.


The paper suggests using a single network with its head split to predict $\hat{y}$ and $\hat{\sigma}^2$. The model does need 'uncertainty labels' to learn the uncertainty. Instead, it infers it from the loss function during the regression task. To make the model work for classification tasks, some adjustments must be made. In their example, the model predicts a vector of unaries, which forms a probability vector when passed through a softmax function. They place a Gaussian distribution over this vector.
Then they approximate through Monte Carlo integration and sample unaries through the softmax function.

All formulas and a better description can be found in the paper in section 3.

As far as I can understand from this paper, the implementation should look like the following:

First, the model's body will be a 'normal' (not bayesian) convolutional network, such as the EEGNet model, which will be explained in the following paper. This model then has two output heads. One which predicts the mean ($\mu$) and one to predict the variance ($\sigma^2$). Each produces a tensor of length 2 (one for each class: ErrP, not Errp). Next, $T$ samples are taken from these distributions and passed to the softmax function. Finally, the loss can be calculated using something like $\beta$-NLL.


\subsection*{EEGNet: a compact convolutional neural network for EEG-based brain--computer interfaces \cite{lawhern2018eegnet}}

This paper proposes a neural network architecture for machine learning models that classify EEG signals, mainly for BCIs. They present an architecture called 'EEGNet.' This architecture is a compact convolutional network (CNN) specifically designed and tailored for EEG signals. These signals generally have a low signal-to-noise ratio and high inter-subject variability. The architecture of EEGNet consists of multiple temporal convolutional layers and a max pooling layer, followed by a spatial convolutional layer and a global averaging layer.

The authors demonstrated the capabilities of this architecture by training and testing it on various tasks and datasets. A few examples are motor imagery tasks and auditory attention tasks. They compared their architecture to various pre-existing architectures and showed that it received higher accuracies than the pre-existing architectures while using fewer parameters. The model was also quite robust to variations; even training runs on small datasets showed promising results.

I will likely use this architecture and convert it to a multi-headed network for my project. Or at least take inspiration from the internal construction of this architecture.

\section*{Papers that are useful for the introduction and comparison with different methods:}

\subsection*{On the pitfalls of heteroscedastic uncertainty estimation with probabilistic neural networks \cite{seitzer2022pitfalls}}

Typically, aleatoric uncertainty in regression tasks can be predicted using a neural network to predict the parameters of the distribution, 
when assuming a heteroscedastic Gaussian distribution.
Usually, using stochastic descent, MLE (Maximum Log Likelihood) is used to learn by minimizing the NLL (Negative Log Likelihood).
This method is pretty much de-facto and was introduced by Nix\cite{nix1994estimating}
However, this method highly depends on the gradients of the predictive variance, creating an unstable model.
This paper suggests an alternative loss function $\beta-NLL$, which counteracts by weighting each data point's contributions to the overall loss by its $\beta$ estimate, 
which controls the dependency of gradients on the predictive variance. Refer back to the paper for the exact theory on how this works.

The GitHub repository contains an implementation of a neural net for a regression task which could be helpful.

\subsection*{Uncertainty quantification for multilabel text classification \cite{chen2020uncertainty}}
They used Bayesian Networks to quantify aleatoric uncertainties in a multilabel text classification environment.
They mainly aimed at capturing the homoscedastic aleatoric uncertainty, which is constant across all inputs. In our case, we are primarily interested in capturing the heteroscedastic aleatoric uncertainty.

\subsection*{Uncertainty estimation in medical image classification: systematic review \cite{kurz2022uncertainty}}
Medical image classification is an essential task in medical diagnosis and treatment planning. However, obtaining large and well-labeled datasets for training deep neural networks in medical image analysis is often challenging, which can lead to uncertainty in the model's predictions. The authors propose a method to estimate the uncertainty in the model's predictions using a Bayesian convolutional neural network. This method resulted in better results than previous state-of-the-art methods.



\subsection*{Inhibited softmax for uncertainty estimation in neural networks \cite{mozejko2018inhibited}}

The author suggests a modified version of the softmax function in this paper, commonly used in classification tasks. This modified version is called the Inhibited softmax function. It takes in another parameter called the inhibition parameter. This parameter controls the degree to which confidence in its prediction is inhibited. The authors show that this modified softmax can predict both aleatoric and epistemic uncertainty.

\subsection*{A deeper look into aleatoric and epistemic uncertainty disentanglement \cite{valdenegro2022deeper}}

For many applications where you wish to predict uncertainty, attempting to disentangle this 'global' uncertainty into its aleatoric and epistemic counterpart can be beneficial. The author reviews existing methods for disentangling this uncertainty, such as maximum likelihood estimation and bayesian deep learning and ensembles. They also propose a technique called Monte Carlo Dropout with Gaussian Smoothing (MCD-GS). This method uses dropout regularization with Gaussian smoothing to estimate both types of uncertainty. 

\subsection*{Optimal Training of Mean Variance Estimation Neural Networks \cite{sluijterman2023optimal}}

The authors propose a novel approach to MVE (Mean Variance Estimation) tasks in this paper. This paper focuses on regression tasks, such as financial and economic predictions, where a mean and variance are desired. They propose two output layers to predict the mean and variance of the distribution separately. The novel contribution of this paper is a new training algorithm that improves the accuracy of the mean and variance estimations. They combine a regularised maximum likelihood objective with a variance clipping method, which stops the network from predicting excessively high or low variances. 

They demonstrate the capabilities of their approach on various MVE regression tasks, such as portfolio optimization and option pricing. They compare them with existing state-of-the-art methods and show that their approach outperforms the existing methods.

\section*{Further possible readings}
\begin{itemize}
    \item BayesNetCNN: incorporating uncertainty in neural networks for image-based classification tasks \cite{ferrante2022bayesnetcnn}
    \item Confidence estimation methods for neural networks: A practical comparison \cite{papadopoulos2001confidence}
    \item Practical uncertainty quantification for brain tumor segmentation \cite{fuchs2021practical}
    \item Reliable multi-class classification based on pairwise epistemic and aleatoric uncertainty\cite{nguyen2018reliable}
\end{itemize}


\bibliographystyle{plain} 
\bibliography{bibfile}

\end{document}